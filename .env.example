# BookNLP GPU Service Configuration
# Copy to .env and modify

# =====================================================
# Model and Data Paths (Host-side)
# =====================================================
# Path on host where BookNLP models will be stored
# This directory will be mounted to /models in the container
MODELS_PATH=/opt/booknlp-models

# Path for input/output data files
DATA_PATH=./data

# Path for temporary files
TEMP_PATH=./temp

# =====================================================
# GPU Configuration
# =====================================================
# Which GPU to use (0, 1, 2, ... or 'all')
GPU_DEVICE=0

# BookNLP model size: 'small' or 'big'
# big: Better accuracy, slower, more VRAM
# small: Faster, less VRAM
BOOKNLP_MODEL=big

# =====================================================
# API Server Settings
# =====================================================
# Port for FastAPI server
API_PORT=8888

# Host to bind (0.0.0.0 for all interfaces)
API_HOST=0.0.0.0

# =====================================================
# Performance Settings
# =====================================================
# Number of worker processes (default: 1)
# Increase if you have multiple texts to process in parallel
WORKERS=1

# Timeout for processing (seconds)
# Increase for very long texts
PROCESSING_TIMEOUT=3600

# Model idle timeout before unloading (seconds)
# Model is loaded on first request (lazy loading)
# and unloaded after this period of inactivity to free GPU memory
# Default: 300 (5 minutes). Set to 0 to disable auto-unload.
MODEL_IDLE_TIMEOUT=300
